"""
Googleæ¤œç´¢æƒ…å ±åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆCustom Search JSON APIä½¿ç”¨ï¼‰
"""

import re
import time
import requests
import random
import os
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse
from googlesearch import search

from core.interfaces import SearchEngineCollector, CollectionResult, SearchResult
from core.exceptions import SearchEngineError
from utils.api_client import OpenAIClient
from utils.execution_logger import ExecutionLogger
from config import config


class GoogleCollector(SearchEngineCollector):
    """Google Custom Search JSON APIã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢çµæœã‹ã‚‰æƒ…å ±ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, delay: float = None, google_api_key: str = None, google_cx: str = None, **kwargs):
        """
        åˆæœŸåŒ–
        
        Args:
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
            google_api_key: Google Custom Search API Key
            google_cx: Google Custom Search Engine ID
        """
        super().__init__(delay or config.search.google_delay, **kwargs)
        self.session = requests.Session()
        
        # Google Custom Search APIè¨­å®š
        self.google_api_key = google_api_key or os.environ.get("GOOGLE_API_KEY")
        self.google_cx = google_cx or os.environ.get("GOOGLE_CX")
        
        # Custom Search JSON API URL
        self.api_base_url = "https://www.googleapis.com/customsearch/v1"
        
        # æ¨™æº–çš„ãªHTTPãƒ˜ãƒƒãƒ€ãƒ¼
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8'
        })
        
        if self.google_api_key and self.google_cx:
            print(f"  Google Custom Search API: æœ‰åŠ¹ (CX: {self.google_cx[:10]}...)")
        else:
            print(f"  Google Custom Search API: ç„¡åŠ¹ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨)")
            print(f"    ğŸ’¡ è¨­å®šæ–¹æ³•: GOOGLE_API_KEY ã¨ GOOGLE_CX ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š")
    
    def collect_info(self, name: str, logger: Optional[ExecutionLogger] = None, api_key: Optional[str] = None, num_results: int = None, **kwargs) -> CollectionResult:
        """
        æŒ‡å®šã•ã‚ŒãŸåå‰ã®äººç‰©æƒ…å ±ã‚’Google Custom Search APIã‹ã‚‰åé›†
        
        Args:
            name: æ¤œç´¢å¯¾è±¡ã®äººç‰©å
            logger: å®Ÿè¡Œãƒ­ã‚°è¨˜éŒ²ç”¨
            api_key: OpenAI API Keyï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            num_results: å–å¾—ã™ã‚‹æ¤œç´¢çµæœæ•°
            
        Returns:
            åé›†ã—ãŸæƒ…å ±
        """
        start_time = time.time()
        num_results = num_results or config.search.google_results
        
        try:
            all_search_results = []
            
            # Custom Search APIãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            if self.google_api_key and self.google_cx:
                # Custom Search APIã‚’ä½¿ç”¨
                search_patterns = self._get_search_patterns(name)
                results_per_pattern = max(1, min(config.search.google_api_results, num_results // len(search_patterns)))  # APIåˆ¶é™è€ƒæ…®
                
                for pattern in search_patterns:
                    print(f"Google Custom Search APIæ¤œç´¢ä¸­: {pattern}")
                    pattern_results = self._search_with_api(pattern, results_per_pattern, name, api_key, logger)
                    all_search_results.extend(pattern_results)
                    
                    # APIåˆ¶é™å¯¾ç­–ã§é©åº¦ãªå¾…æ©Ÿ
                    time.sleep(self.delay)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ¤œç´¢æ–¹æ³•
                print("âš ï¸  Google Custom Search APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                search_patterns = self._get_search_patterns(name)
                results_per_pattern = max(1, num_results // len(search_patterns))
                
                for pattern in search_patterns:
                    print(f"Google ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ä¸­: {pattern}")
                    pattern_results = self._search_single_pattern_fallback(pattern, results_per_pattern, name, api_key, logger)
                    all_search_results.extend(pattern_results)
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã§å¤§å¹…å¾…æ©Ÿ
                    time.sleep(self.delay * 3)
            
            duration = time.time() - start_time
            query_description = "è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ï¼ˆCustom Search APIï¼‰" if self.google_api_key else "è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"
            
            # SearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            search_result_objects = []
            for result_dict in all_search_results:
                try:
                    search_result = SearchResult(
                        url=result_dict.get("url", ""),
                        title=result_dict.get("title", ""),
                        description=result_dict.get("description", ""),
                        content=result_dict.get("content", ""),
                        domain=result_dict.get("domain", ""),
                        content_length=result_dict.get("content_length", 0),
                        speech_patterns=result_dict.get("speech_patterns", []),
                        source="google",
                        search_query=query_description
                    )
                    search_result_objects.append(search_result)
                except Exception as e:
                    if logger:
                        logger.log_error("search_result_conversion_error", str(e), result_dict)
                    continue
            
            return self._create_success_result(search_result_objects, query_description, duration)
            
        except Exception as e:
            return self._create_error_result(f"Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}", "è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢")
    
    def _get_search_patterns(self, name: str) -> List[str]:
        """æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ"""
        patterns = [
            f'"{name}"',
            f'{name} ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼',
            f'{name} å£èª¿',
            f'{name} è©±ã—æ–¹',
            f'{name} ã‚»ãƒªãƒ•'
        ]
        return patterns
    
    def _search_with_api(self, search_query: str, max_results: int, character_name: str = "", api_key: str = None, logger=None) -> List[Dict[str, Any]]:
        """
        Google Custom Search APIã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢ã‚’å®Ÿè¡Œ
        
        Args:
            search_query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            max_results: æœ€å¤§å–å¾—çµæœæ•°
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            api_key: OpenAI API Key
            logger: ãƒ­ã‚°è¨˜éŒ²ç”¨
            
        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        search_results = []
        
        try:
            # Custom Search API ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            params = {
                'key': self.google_api_key,
                'cx': self.google_cx,
                'q': search_query,
                'num': min(max_results, 10),  # APIã¯æœ€å¤§10ä»¶ã¾ã§ï¼ˆGoogleåˆ¶é™ï¼‰
                'lr': 'lang_ja',  # æ—¥æœ¬èªçµæœã‚’å„ªå…ˆ
                'gl': 'jp',  # æ—¥æœ¬ã‹ã‚‰ã®æ¤œç´¢ã¨ã—ã¦å®Ÿè¡Œ
                'safe': 'off'  # ã‚»ãƒ¼ãƒ•ã‚µãƒ¼ãƒã‚ªãƒ•
            }
            
            print(f"    APIæ¤œç´¢å®Ÿè¡Œ: {search_query}")
            response = self.session.get(self.api_base_url, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                
                if 'items' in data:
                    print(f"    APIçµæœ: {len(data['items'])}ä»¶å–å¾—")
                    
                    for item in data['items']:
                        try:
                            url = item.get('link', '')
                            title = item.get('title', '')
                            snippet = item.get('snippet', '')
                            
                            if url and title:
                                # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’è©³ç´°å–å¾—
                                content_info = self._extract_page_content(url, character_name, api_key, logger)
                                if content_info:
                                    # APIçµæœã®æƒ…å ±ã‚‚ãƒãƒ¼ã‚¸
                                    content_info['api_title'] = title
                                    content_info['api_snippet'] = snippet
                                    search_results.append(content_info)
                                else:
                                    # ãƒšãƒ¼ã‚¸å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯APIçµæœã®ã¿ä½¿ç”¨
                                    content_info = {
                                        "url": url,
                                        "domain": self._extract_domain(url),
                                        "title": title,
                                        "description": snippet,
                                        "content": snippet,
                                        "content_length": len(snippet),
                                        "speech_patterns": self._extract_basic_patterns(snippet + " " + title, character_name)
                                    }
                                    search_results.append(content_info)
                                
                                # ãƒšãƒ¼ã‚¸é–“ã®é©åº¦ãªå¾…æ©Ÿ
                                time.sleep(self.delay)
                                
                        except Exception as e:
                            print(f"    APIçµæœå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                else:
                    print(f"    APIçµæœ: æ¤œç´¢çµæœãªã—")
                    
            elif response.status_code == 403:
                print(f"    APIåˆ¶é™ã‚¨ãƒ©ãƒ¼: æ—¥æ¬¡ã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯API Keyç„¡åŠ¹")
                if logger:
                    logger.log_error("google_api_quota_error", "Google Custom Search API quota exceeded", {
                        "search_query": search_query,
                        "status_code": response.status_code,
                        "response": response.text[:200]
                    })
            else:
                print(f"    APIæ¤œç´¢å¤±æ•—: HTTP {response.status_code}")
                if logger:
                    logger.log_error("google_api_error", f"Google Custom Search API error: {response.status_code}", {
                        "search_query": search_query,
                        "status_code": response.status_code,
                        "response": response.text[:200]
                    })
                        
        except Exception as e:
            print(f"    APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({search_query}): {e}")
            if logger:
                logger.log_error("google_api_exception", str(e), {
                    "search_query": search_query,
                    "error_type": type(e).__name__
                })
        
        return search_results
    
    
    def _search_single_pattern_fallback(self, search_query: str, max_results: int, character_name: str = "", api_key: str = None, logger=None) -> List[Dict[str, Any]]:
        """
        å˜ä¸€ã®æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Ÿè¡Œ
        
        Args:
            search_query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            max_results: æœ€å¤§å–å¾—çµæœæ•°
            
        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        search_results = []
        
        max_retries = 3
        retry_delay = 30  # 30ç§’å¾…æ©Ÿ
        
        for retry in range(max_retries):
            try:
                # Googleæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆUser-Agentè¨­å®šä»˜ãï¼‰
                search_urls = []
                
                # googlesearch-pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«è¿½åŠ ã®è¨­å®šã‚’è©¦è¡Œ
                try:
                    # ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“ã§ã‚ˆã‚Šäººé–“ã‚‰ã—ã„å‹•ä½œã‚’æ¨¡å€£
                    base_delay = max(self.delay * 3, 10.0)  # æœ€ä½10ç§’å¾…æ©Ÿ
                    
                    print(f"    Googleæ¤œç´¢å®Ÿè¡Œä¸­ï¼ˆ{base_delay}ç§’é–“éš”ã§æ…é‡ã«å–å¾—ï¼‰...")
                    
                    # googlesearch-pythonã®User-Agentè¨­å®šã‚’è©¦è¡Œ
                    import googlesearch
                    
                    # ã‚ˆã‚Šä¿å®ˆçš„ãªè¨­å®šã§æ¤œç´¢ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–å¼·åŒ–ï¼‰
                    search_timeout = False
                    start_search_time = time.time()
                    
                    for url in search(
                        search_query, 
                        num=max_results, 
                        stop=max_results, 
                        pause=base_delay,
                        lang='ja',  # æ—¥æœ¬èªæ¤œç´¢ã‚’æ˜ç¤º
                        safe='off',  # ã‚»ãƒ¼ãƒ•ã‚µãƒ¼ãƒã‚ªãƒ•
                        country='jp'  # æ—¥æœ¬ã‹ã‚‰ã®æ¤œç´¢ã‚’æ˜ç¤º
                    ):
                        # æ¤œç´¢æ™‚é–“ã®ãƒã‚§ãƒƒã‚¯ï¼ˆ60ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
                        if time.time() - start_search_time > 60:
                            print(f"    âš ï¸ æ¤œç´¢å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ60ç§’ï¼‰ã®ãŸã‚ä¸­æ–­ã—ã¾ã™")
                            search_timeout = True
                            break
                        search_urls.append(url)
                        print(f"    URLå–å¾—: {url}")
                        
                        if len(search_urls) >= max_results:
                            break
                        
                        # è¿½åŠ ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿï¼ˆBotæ¤œå‡ºå›é¿ï¼‰
                        extra_delay = random.uniform(2.0, 5.0)
                        total_delay = base_delay + extra_delay
                        print(f"    æ¬¡ã®URLå–å¾—ã¾ã§ {total_delay:.1f}ç§’å¾…æ©Ÿ...")
                        time.sleep(extra_delay)
                    
                    if search_timeout:
                        print(f"    âš ï¸ æ¤œç´¢ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚å–å¾—æ¸ˆã¿URL: {len(search_urls)}ä»¶")
                        if logger:
                            logger.log_error("google_search_timeout", "æ¤œç´¢å‡¦ç†ãŒ60ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ", {
                                "search_query": search_query,
                                "urls_collected": len(search_urls)
                            })
                        
                except Exception as search_lib_error:
                    print(f"    googlesearch-pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¨ãƒ©ãƒ¼: {search_lib_error}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç›´æ¥Googleæ¤œç´¢ã‚’è©¦è¡Œ
                    search_urls = self._fallback_google_search(search_query, max_results)
                
                # å„URLã‹ã‚‰æƒ…å ±ã‚’å–å¾—
                for url in search_urls:
                    try:
                        # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—
                        content_info = self._extract_page_content(url, character_name, api_key, logger)
                        if content_info:
                            search_results.append(content_info)
                        
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã§å¤§å¹…å¾…æ©Ÿ
                        time.sleep(self.delay * 2)
                        
                    except Exception as e:
                        print(f"URLå–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                        if logger:
                            logger.log_error("url_extraction_error", str(e), {"url": url, "search_query": search_query})
                        continue
                
                # æˆåŠŸã—ãŸå ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                break
                        
            except Exception as search_error:
                error_str = str(search_error)
                print(f"æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({search_query}): {search_error}")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                if "429" in error_str or "Too Many Requests" in error_str:
                    if logger:
                        logger.log_error("google_rate_limit", f"Googleæ¤œç´¢ãƒ¬ãƒ¼ãƒˆåˆ¶é™ (è©¦è¡Œ{retry+1}/{max_retries})", {
                            "search_query": search_query,
                            "retry_attempt": retry + 1,
                            "max_retries": max_retries,
                            "error_details": error_str
                        })
                        
                    print(f"âš ï¸  Googleæ¤œç´¢ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                    print(f"    æ¤œç´¢ã‚¯ã‚¨ãƒª: {search_query}")
                    print(f"    è©¦è¡Œå›æ•°: {retry + 1}/{max_retries}")
                    
                    if retry < max_retries - 1:
                        wait_time = retry_delay * (retry + 1)  # æ®µéšçš„ã«å¾…æ©Ÿæ™‚é–“ã‚’å¢—åŠ 
                        print(f"    {wait_time}ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        print(f"    ğŸ’¡ é »ç¹ã«ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’ãŠè©¦ã—ãã ã•ã„:")
                        print(f"       - --use-bing ãƒ•ãƒ©ã‚°ã§Bingæ¤œç´¢ã‚’ä½¿ç”¨")
                        print(f"       - --no-google ãƒ•ãƒ©ã‚°ã§Webæ¤œç´¢ã‚’ç„¡åŠ¹åŒ–")
                        print(f"       - ã—ã°ã‚‰ãæ™‚é–“ã‚’ç½®ã„ã¦ã‹ã‚‰å†å®Ÿè¡Œ")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸã€‚æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {search_query}")
                        print(f"    ğŸ’¡ Googleæ¤œç´¢ãŒç¶™ç¶šçš„ã«å¤±æ•—ã™ã‚‹å ´åˆã®å¯¾å‡¦æ³•:")
                        print(f"       1. --use-bing ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ã¦Bingæ¤œç´¢ã«åˆ‡ã‚Šæ›¿ãˆ")
                        print(f"       2. --no-google ãƒ•ãƒ©ã‚°ã§Webæ¤œç´¢ã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ–")
                        print(f"       3. æ•°æ™‚é–“å¾Œã«å†è©¦è¡Œï¼ˆGoogleå´ã®åˆ¶é™ãƒªã‚»ãƒƒãƒˆå¾…ã¡ï¼‰")
                        if logger:
                            logger.log_error("google_rate_limit_exceeded", f"Googleæ¤œç´¢ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¶…é", {
                                "search_query": search_query,
                                "max_retries": max_retries
                            })
                        break
                else:
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒªãƒˆãƒ©ã‚¤ã—ãªã„
                    if logger:
                        logger.log_error("google_search_error", str(search_error), {"search_query": search_query})
                    break
        
        return search_results
    
    def _fallback_google_search(self, query: str, max_results: int) -> List[str]:
        """
        googlesearch-pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            max_results: æœ€å¤§çµæœæ•°
            
        Returns:
            æ¤œç´¢çµæœURLã®ãƒªã‚¹ãƒˆ
        """
        print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œä¸­: {query}")
        urls = []
        
        try:
            # åŸºæœ¬çš„ãªGoogleæ¤œç´¢URLæ§‹ç¯‰
            import urllib.parse
            encoded_query = urllib.parse.quote_plus(query)
            google_url = f"https://www.google.com/search?q={encoded_query}&num={max_results}&hl=ja"
            
            print(f"    ç›´æ¥Googleæ¤œç´¢URL: {google_url}")
            
            response = self.session.get(google_url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Googleæ¤œç´¢çµæœã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                for link in soup.find_all('a'):
                    href = link.get('href')
                    if href and href.startswith('/url?q='):
                        # Googleã®å†…éƒ¨URLã‹ã‚‰å®Ÿéš›ã®URLã‚’æŠ½å‡º
                        actual_url = href.split('/url?q=')[1].split('&')[0]
                        actual_url = urllib.parse.unquote(actual_url)
                        
                        # æœ‰åŠ¹ãªURLã®ã¿ã‚’è¿½åŠ 
                        if actual_url.startswith('http') and 'google.com' not in actual_url:
                            urls.append(actual_url)
                            if len(urls) >= max_results:
                                break
                
                print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢çµæœ: {len(urls)}ä»¶")
            else:
                print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢å¤±æ•—: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return urls
    
    def _extract_page_content(self, url: str, character_name: str = "", api_key: str = None, logger=None) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒšãƒ¼ã‚¸å†…å®¹ã‚’æŠ½å‡º
        
        Args:
            url: å¯¾è±¡URL
            
        Returns:
            æŠ½å‡ºã—ãŸå†…å®¹ã®è¾æ›¸
        """
        try:
            # URLã®æ¤œè¨¼ã¨ä¿®æ­£
            if not url.startswith(('http://', 'https://')):
                if url.startswith('//'):
                    url = 'https:' + url
                elif url.startswith('/'):
                    return None  # ç›¸å¯¾URLã¯ç„¡åŠ¹ã¨ã—ã¦é™¤å¤–
                else:
                    url = 'https://' + url
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            domain = self._extract_domain(url)  # ãƒ‰ãƒ¡ã‚¤ãƒ³æƒ…å ±ã‚’å…ˆã«å–å¾—
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "ä¸æ˜"
            
            # ãƒ¡ã‚¿èª¬æ˜ã‚’å–å¾—
            meta_description = soup.find('meta', attrs={'name': 'description'})
            description = meta_description.get('content', '') if meta_description else ''
            
            # æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆä¸»è¦ãªéƒ¨åˆ†ã®ã¿ï¼‰
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
            for script in soup(["script", "style", "nav", "header", "footer"]):
                script.decompose()
            
            # æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            body_text = soup.get_text()
            # ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»
            lines = (line.strip() for line in body_text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            body_text = ' '.join(chunk for chunk in chunks if chunk)
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¶é™
            body_text = body_text[:config.search.google_page_limit] if body_text else ""
            
            # å£èª¿ãƒ»ã‚»ãƒªãƒ•é–¢é€£ã®æ–‡ã‚’ChatGPT APIã§æŠ½å‡ºï¼ˆAPI keyãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
            speech_patterns = []
            if api_key and len(body_text.strip()) > 50:  # ååˆ†ãªãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿
                try:
                    openai_client = OpenAIClient(api_key)
                    speech_patterns = openai_client.extract_speech_patterns(body_text, character_name, logger)
                except Exception as api_error:
                    print(f"    APIæŠ½å‡ºã‚¹ã‚­ãƒƒãƒ— ({domain}): {api_error}")
                    if logger:
                        logger.log_error("speech_pattern_api_error", str(api_error), {
                            "url": url,
                            "domain": domain,
                            "character_name": character_name,
                            "content_length": len(body_text)
                        })
                    speech_patterns = []
            
            return {
                "url": url,
                "domain": domain,
                "title": title_text,
                "description": description,
                "content": body_text,
                "content_length": len(body_text),
                "speech_patterns": speech_patterns
            }
            
        except requests.RequestException as e:
            print(f"HTTPå–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            if logger:
                logger.log_error("http_request_error", str(e), {"url": url, "error_type": "RequestException"})
            return None
        except Exception as e:
            print(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            if logger:
                logger.log_error("content_extraction_error", str(e), {"url": url, "error_type": type(e).__name__})
            return None
    
    def search_youtube_videos(self, name: str) -> List[str]:
        """
        YouTubeå‹•ç”»ã‚’æ¤œç´¢ï¼ˆå‹•ç”»URLã‚’å„ªå…ˆçš„ã«å–å¾—ï¼‰
        
        Args:
            name: æ¤œç´¢å¯¾è±¡ã®åå‰
            
        Returns:
            YouTubeå‹•ç”»URLã®ãƒªã‚¹ãƒˆ
        """
        try:
            # ã‚ˆã‚Šç²¾å¯†ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
            search_queries = []
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã®ã¿ã®å˜ç´”ãªæ¤œç´¢
            if len(name) > 2:  # åå‰ãŒçŸ­ã™ãã‚‹å ´åˆã®èª¤ãƒ’ãƒƒãƒˆé˜²æ­¢
                search_queries.extend([
                    f'"{name}" site:youtube.com'
                ])
            
            # ä¸€èˆ¬çš„ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã®ã¿ï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç‰¹æœ‰ã®æƒ…å ±ã¯å«ã‚ãªã„ï¼‰
            
            # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å»ƒæ­¢ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šï¼‰
            
            youtube_urls = []
            
            for search_query in search_queries:
                if len(youtube_urls) >= config.search.youtube_max_urls:
                    break
                    
                print(f"YouTubeæ¤œç´¢ä¸­: {search_query}")
                
                try:
                    # Custom Search APIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯APIã‚’ä½¿ç”¨
                    if self.google_api_key and self.google_cx:
                        api_urls = self._search_youtube_with_api(search_query)
                        for url in api_urls:
                            if url not in youtube_urls:
                                youtube_urls.append(url)
                                print(f"  - å‹•ç”»URLç™ºè¦‹ï¼ˆAPIï¼‰: {url}")
                            if len(youtube_urls) >= config.search.youtube_max_urls:
                                break
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ¤œç´¢æ–¹æ³•
                        for url in search(search_query):
                            # å‹•ç”»URLã‚’åé›†
                            if 'youtube.com/watch?v=' in url and url not in youtube_urls:
                                youtube_urls.append(url)
                                print(f"  - å‹•ç”»URLç™ºè¦‹: {url}")
                            
                            if len(youtube_urls) >= config.search.youtube_max_urls:
                                break
                            time.sleep(config.search.youtube_search_delay)
                        
                except Exception as search_error:
                    print(f"YouTubeæ¤œç´¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({search_query}): {search_error}")
                    # YouTubeæ¤œç´¢ã‚¨ãƒ©ãƒ¼ã¯loggerã«è¨˜éŒ²ã—ãªã„ï¼ˆé€šå¸¸ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ãªã„ãŸã‚ï¼‰
                    continue
                
                # ã‚¯ã‚¨ãƒªé–“ã®å¤§å¹…å¾…æ©Ÿï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰
                time.sleep(self.delay * 4)
            
            print(f"YouTubeå‹•ç”»URLå–å¾—å®Œäº†: {len(youtube_urls)}ä»¶")
            return youtube_urls
            
        except Exception as e:
            print(f"YouTubeæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            # YouTubeæ¤œç´¢ã‚¨ãƒ©ãƒ¼ã¯loggerã«è¨˜éŒ²ã—ãªã„ï¼ˆé€šå¸¸ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ãªã„ãŸã‚ï¼‰
            return []
    
    def _search_youtube_with_api(self, search_query: str) -> List[str]:
        """
        Custom Search APIã‚’ä½¿ç”¨ã—ã¦YouTubeå‹•ç”»ã‚’æ¤œç´¢
        
        Args:
            search_query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            
        Returns:
            YouTubeå‹•ç”»URLã®ãƒªã‚¹ãƒˆ
        """
        youtube_urls = []
        
        try:
            # Custom Search API ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆYouTubeæ¤œç´¢ï¼‰
            params = {
                'key': self.google_api_key,
                'cx': self.google_cx,
                'q': search_query,
                'num': 10,  # æœ€å¤§10ä»¶
                'lr': 'lang_ja',  # æ—¥æœ¬èªçµæœã‚’å„ªå…ˆ
                'gl': 'jp',  # æ—¥æœ¬ã‹ã‚‰ã®æ¤œç´¢ã¨ã—ã¦å®Ÿè¡Œ
                'safe': 'off',
                'siteSearch': 'youtube.com'  # YouTubeé™å®šæ¤œç´¢
            }
            
            response = self.session.get(self.api_base_url, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                
                if 'items' in data:
                    for item in data['items']:
                        url = item.get('link', '')
                        if 'youtube.com/watch?v=' in url:
                            youtube_urls.append(url)
                        
                        if len(youtube_urls) >= config.search.youtube_max_urls:
                            break
                            
        except Exception as e:
            print(f"    YouTube APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return youtube_urls
    
    def _extract_title_from_url(self, url: str) -> str:
        """
        URLã‹ã‚‰ç°¡å˜ã«ã‚¿ã‚¤ãƒˆãƒ«æƒ…å ±ã‚’æŠ½å‡ºï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        
        Args:
            url: YouTube URL
            
        Returns:
            ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—åˆ—ï¼ˆå–å¾—ã§ããªã„å ´åˆã¯ç©ºæ–‡å­—ï¼‰
        """
        # å®Ÿéš›ã®ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ã¯é‡ã™ãã‚‹ã®ã§ã€ç©ºæ–‡å­—ã‚’è¿”ã™
        return ""
    
    
