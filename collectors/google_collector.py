"""
Googleæ¤œç´¢æƒ…å ±åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆCustom Search JSON APIä½¿ç”¨ï¼‰
"""

import re
import time
import requests
import random
import os
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse
from googlesearch import search

from core.interfaces import SearchEngineCollector, CollectionResult, SearchResult
from core.exceptions import SearchEngineError
from utils.api_client import OpenAIClient
from utils.execution_logger import ExecutionLogger
from config import (
    GOOGLE_DELAY, GOOGLE_RESULTS, GOOGLE_API_RESULTS, GOOGLE_PAGE_LIMIT,
    YOUTUBE_MAX_URLS, YOUTUBE_SEARCH_DELAY, GOOGLE_FALLBACK_DELAY_MULTIPLIER,
    GOOGLE_MIN_DELAY, GOOGLE_URL_FETCH_DELAY, GOOGLE_PAGE_DELAY_MULTIPLIER,
    GOOGLE_YOUTUBE_DELAY_MULTIPLIER, GOOGLE_CX_DISPLAY_LENGTH,
    GOOGLE_API_MAX_RESULTS_PER_REQUEST, GOOGLE_FALLBACK_MAX_RETRIES,
    GOOGLE_FALLBACK_RETRY_DELAY, GOOGLE_PATTERN_TIMEOUT, GOOGLE_SEARCH_TIMEOUT,
    GOOGLE_MIN_TEXT_LENGTH_FOR_API, GOOGLE_YOUTUBE_API_RESULTS, REQUEST_TIMEOUT,
    HTTP_STATUS_OK, HTTP_STATUS_FORBIDDEN, HTTP_STATUS_TOO_MANY_REQUESTS,
    SAMPLE_QUALITY_MIN_LENGTH, BING_NAME_LENGTH_CHECK, MAX_RETRIES,
    DEFAULT_USER_AGENT, PREVIEW_LENGTH_LONG, THREAD_POOL_MAX_WORKERS_SINGLE,
    GOOGLE_429_EXTRA_DELAY, GOOGLE_FETCH_PAGE_CONTENT, HTTP_STATUS_NOT_FOUND
)


class GoogleCollector(SearchEngineCollector):
    """Google Custom Search JSON APIã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢çµæœã‹ã‚‰æƒ…å ±ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, delay: float = None, google_api_key: str = None, google_cx: str = None, **kwargs):
        """
        åˆæœŸåŒ–
        
        Args:
            delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
            google_api_key: Google Custom Search API Key
            google_cx: Google Custom Search Engine ID
        """
        super().__init__(delay or GOOGLE_DELAY, **kwargs)
        self.session = requests.Session()
        
        # Google Custom Search APIè¨­å®š
        self.google_api_key = google_api_key or os.environ.get("GOOGLE_API_KEY")
        self.google_cx = google_cx or os.environ.get("GOOGLE_CX")
        
        # Custom Search JSON API URL
        self.api_base_url = "https://www.googleapis.com/customsearch/v1"
        
        # æ¨™æº–çš„ãªHTTPãƒ˜ãƒƒãƒ€ãƒ¼
        self.session.headers.update({
            'User-Agent': DEFAULT_USER_AGENT,
            'Accept': 'application/json',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8'
        })
        
        if self.google_api_key and self.google_cx:
            print(f"  Google Custom Search API: æœ‰åŠ¹ (CX: {self.google_cx[:GOOGLE_CX_DISPLAY_LENGTH]}...)")
        else:
            print(f"  Google Custom Search API: ç„¡åŠ¹ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨)")
            print(f"    ğŸ’¡ è¨­å®šæ–¹æ³•: GOOGLE_API_KEY ã¨ GOOGLE_CX ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š")
    
    
    def collect_info(self, name: str, logger: Optional[ExecutionLogger] = None, api_key: Optional[str] = None, num_results: int = None, **kwargs) -> CollectionResult:
        """
        æŒ‡å®šã•ã‚ŒãŸåå‰ã®äººç‰©æƒ…å ±ã‚’Google Custom Search APIã‹ã‚‰åé›†
        
        Args:
            name: æ¤œç´¢å¯¾è±¡ã®äººç‰©å
            logger: å®Ÿè¡Œãƒ­ã‚°è¨˜éŒ²ç”¨
            api_key: OpenAI API Keyï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            num_results: å–å¾—ã™ã‚‹æ¤œç´¢çµæœæ•°
            
        Returns:
            åé›†ã—ãŸæƒ…å ±
        """
        start_time = time.time()
        num_results = num_results or GOOGLE_RESULTS
        
        try:
            all_search_results = []
            
            # Custom Search APIãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            if self.google_api_key and self.google_cx:
                print(f"    Google Custom Search APIã‚’ä½¿ç”¨")
                # Custom Search APIã‚’ä½¿ç”¨
                search_patterns = self._get_search_patterns(name)
                print(f"    {len(search_patterns)}å€‹ã®æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨")
                results_per_pattern = max(1, min(GOOGLE_API_RESULTS, num_results // len(search_patterns)))  # APIåˆ¶é™è€ƒæ…®
                
                for i, pattern in enumerate(search_patterns):
                    print(f"\n    ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}/{len(search_patterns)}: '{pattern}'")
                    pattern_results = self._search_with_api(pattern, results_per_pattern, name, api_key, logger)
                    all_search_results.extend(pattern_results)
                    print(f"      âœ… {len(pattern_results)}ä»¶ã®çµæœã‚’å–å¾—")
                    
                    # APIåˆ¶é™å¯¾ç­–ã§é©åº¦ãªå¾…æ©Ÿ
                    time.sleep(self.delay)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ¤œç´¢æ–¹æ³•
                print("âš ï¸  Google Custom Search APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                search_patterns = self._get_search_patterns(name)
                print(f"    {len(search_patterns)}å€‹ã®æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨")
                results_per_pattern = max(1, num_results // len(search_patterns))
                
                for i, pattern in enumerate(search_patterns):
                    print(f"\n    ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}/{len(search_patterns)}: '{pattern}'")
                    pattern_results = self._search_single_pattern_fallback(pattern, results_per_pattern, name, api_key, logger)
                    all_search_results.extend(pattern_results)
                    print(f"      âœ… {len(pattern_results)}ä»¶ã®çµæœã‚’å–å¾—")
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã§å¤§å¹…å¾…æ©Ÿ
                    time.sleep(self.delay * GOOGLE_FALLBACK_DELAY_MULTIPLIER)
            
            duration = time.time() - start_time
            query_description = "è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ï¼ˆCustom Search APIï¼‰" if self.google_api_key else "è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"
            
            # SearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            search_result_objects = []
            for result_dict in all_search_results:
                try:
                    search_result = SearchResult(
                        url=result_dict.get("url", ""),
                        title=result_dict.get("title", ""),
                        description=result_dict.get("description", ""),
                        content=result_dict.get("content", ""),
                        domain=result_dict.get("domain", ""),
                        content_length=result_dict.get("content_length", 0),
                        speech_patterns=result_dict.get("speech_patterns", []),
                        source="google",
                        search_query=query_description
                    )
                    search_result_objects.append(search_result)
                except Exception as e:
                    if logger:
                        logger.log_error("search_result_conversion_error", str(e), result_dict)
                    continue
            
            return self._create_success_result(search_result_objects, query_description, duration)
            
        except Exception as e:
            return self._create_error_result(f"Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}", "è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢")
    
    def _get_search_patterns(self, name: str) -> List[str]:
        """æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ"""
        patterns = [
            f'"{name}"',
            f'{name} ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼',
            f'{name} å£èª¿',
            f'{name} è©±ã—æ–¹',
            f'{name} ã‚»ãƒªãƒ•'
        ]
        return patterns
    
    def _search_with_api(self, search_query: str, max_results: int, character_name: str = "", api_key: str = None, logger=None) -> List[Dict[str, Any]]:
        """
        Google Custom Search APIã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢ã‚’å®Ÿè¡Œ
        
        Args:
            search_query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            max_results: æœ€å¤§å–å¾—çµæœæ•°
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            api_key: OpenAI API Key
            logger: ãƒ­ã‚°è¨˜éŒ²ç”¨
            
        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        search_results = []
        
        try:
            # Custom Search API ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            params = {
                'key': self.google_api_key,
                'cx': self.google_cx,
                'q': search_query,
                'num': min(max_results, GOOGLE_API_MAX_RESULTS_PER_REQUEST),  # APIã¯æœ€å¤§ä»¶æ•°ã¾ã§ï¼ˆGoogleåˆ¶é™ï¼‰
                'lr': 'lang_ja',  # æ—¥æœ¬èªçµæœã‚’å„ªå…ˆ
                'gl': 'jp',  # æ—¥æœ¬ã‹ã‚‰ã®æ¤œç´¢ã¨ã—ã¦å®Ÿè¡Œ
                'safe': 'off'  # ã‚»ãƒ¼ãƒ•ã‚µãƒ¼ãƒã‚ªãƒ•
            }
            
            print(f"      Google Custom Search APIãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
            print(f"      API Key: {self.google_api_key[:10]}... (length: {len(self.google_api_key)})")
            print(f"      CX: {self.google_cx[:GOOGLE_CX_DISPLAY_LENGTH] if self.google_cx else 'None'}... (length: {len(self.google_cx) if self.google_cx else 0})")
            
            response = self.session.get(self.api_base_url, params=params, timeout=REQUEST_TIMEOUT)
            
            if response.status_code == HTTP_STATUS_OK:
                data = response.json()
                
                if 'items' in data:
                    print(f"      APIã‹ã‚‰{len(data['items'])}ä»¶ã®çµæœã‚’å–å¾—")
                    
                    for item in data['items']:
                        try:
                            url = item.get('link', '')
                            title = item.get('title', '')
                            snippet = item.get('snippet', '')
                            
                            if url and title:
                                if GOOGLE_FETCH_PAGE_CONTENT:
                                    # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’è©³ç´°å–å¾—
                                    print(f"        ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {url[:50]}...")
                                    content_info = self._extract_page_content(url, character_name, api_key, logger)
                                    if content_info:
                                        # APIçµæœã®æƒ…å ±ã‚‚ãƒãƒ¼ã‚¸
                                        content_info['api_title'] = title
                                        content_info['api_snippet'] = snippet
                                        search_results.append(content_info)
                                    else:
                                        # ãƒšãƒ¼ã‚¸å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯APIçµæœã®ã¿ä½¿ç”¨
                                        content_info = {
                                            "url": url,
                                            "domain": self._extract_domain(url),
                                            "title": title,
                                            "description": snippet,
                                            "content": snippet,
                                            "content_length": len(snippet),
                                            "speech_patterns": self._extract_basic_patterns(snippet + " " + title, character_name)
                                        }
                                        search_results.append(content_info)
                                else:
                                    # ãƒšãƒ¼ã‚¸å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦APIçµæœã®ã¿ä½¿ç”¨ï¼ˆé«˜é€ŸåŒ–ï¼‰
                                    content_info = {
                                        "url": url,
                                        "domain": self._extract_domain(url),
                                        "title": title,
                                        "description": snippet,
                                        "content": snippet,
                                        "content_length": len(snippet),
                                        "speech_patterns": []
                                    }
                                    search_results.append(content_info)
                                
                                # ãƒšãƒ¼ã‚¸é–“ã®é©åº¦ãªå¾…æ©Ÿ
                                time.sleep(self.delay)
                                
                        except Exception as e:
                            print(f"    APIçµæœå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                else:
                    print(f"      âš ï¸ æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                    
            elif response.status_code == HTTP_STATUS_TOO_MANY_REQUESTS:
                print(f"      âŒ Google Custom Search APIã®æ—¥æ¬¡ã‚¯ã‚©ãƒ¼ã‚¿ã‚’è¶…éã—ã¾ã—ãŸï¼ˆ429ã‚¨ãƒ©ãƒ¼ï¼‰")
                print(f"      ğŸ’¡ å¯¾å‡¦æ³•:")
                print(f"         1. --use-bing ãƒ•ãƒ©ã‚°ã§Bingæ¤œç´¢ã‚’ä½¿ç”¨")
                print(f"         2. --use-chatgpt-search ãƒ•ãƒ©ã‚°ã§ChatGPTçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨")
                print(f"         3. --no-google ãƒ•ãƒ©ã‚°ã§Googleæ¤œç´¢ã‚’ç„¡åŠ¹åŒ–")
                print(f"         4. ç¿Œæ—¥ã¾ã§å¾…ã¤ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿ã¯æ—¥æœ¬æ™‚é–“åˆå‰0æ™‚ã«ãƒªã‚»ãƒƒãƒˆï¼‰")
            elif response.status_code == HTTP_STATUS_FORBIDDEN:
                print(f"      âŒ APIèªè¨¼ã‚¨ãƒ©ãƒ¼: API KeyãŒç„¡åŠ¹ã¾ãŸã¯è¨­å®šãƒŸã‚¹")
                if logger:
                    logger.log_error("google_api_quota_error", "Google Custom Search API quota exceeded", {
                        "search_query": search_query,
                        "status_code": response.status_code,
                        "response": response.text[:PREVIEW_LENGTH_LONG]
                    })
            else:
                print(f"      âŒ APIæ¤œç´¢å¤±æ•—: HTTP {response.status_code}")
                if response.status_code == HTTP_STATUS_TOO_MANY_REQUESTS:
                    print(f"      âŒ Google Custom Search APIã®æ—¥æ¬¡ã‚¯ã‚©ãƒ¼ã‚¿ã‚’è¶…éã—ã¾ã—ãŸ")
                    print(f"      ğŸ’¡ å¯¾å‡¦æ³•:")
                    print(f"         1. --use-bing ãƒ•ãƒ©ã‚°ã§Bingæ¤œç´¢ã‚’ä½¿ç”¨")
                    print(f"         2. --use-chatgpt-search ãƒ•ãƒ©ã‚°ã§ChatGPTçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨")
                    print(f"         3. --no-google ãƒ•ãƒ©ã‚°ã§Googleæ¤œç´¢ã‚’ç„¡åŠ¹åŒ–")
                    print(f"         4. ç¿Œæ—¥ã¾ã§å¾…ã¤ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿ã¯æ—¥æœ¬æ™‚é–“åˆå‰0æ™‚ã«ãƒªã‚»ãƒƒãƒˆï¼‰")
                elif response.status_code == HTTP_STATUS_NOT_FOUND:
                    print(f"      âŒ Google Custom Search Engine ID (CX) ãŒç„¡åŠ¹ã§ã™")
                    print(f"      ğŸ’¡ å¯¾å‡¦æ³•:")
                    print(f"         1. Google Custom Search Engineã‚’ä½œæˆ: https://programmablesearchengine.google.com/")
                    print(f"         2. ç’°å¢ƒå¤‰æ•° GOOGLE_CX ã«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³IDã‚’è¨­å®š")
                    print(f"         3. ã¾ãŸã¯ --use-bing / --use-chatgpt-search ã‚’ä½¿ç”¨")
                    print(f"      ç¾åœ¨ã®CX: {self.google_cx if self.google_cx else 'æœªè¨­å®š'}")
                if logger:
                    logger.log_error("google_api_error", f"Google Custom Search API error: {response.status_code}", {
                        "search_query": search_query,
                        "status_code": response.status_code,
                        "response": response.text[:PREVIEW_LENGTH_LONG]
                    })
                        
        except Exception as e:
            print(f"      âŒ APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            if logger:
                logger.log_error("google_api_exception", str(e), {
                    "search_query": search_query,
                    "error_type": type(e).__name__
                })
        
        return search_results
    
    
    def _search_single_pattern_fallback(self, search_query: str, max_results: int, character_name: str = "", api_key: str = None, logger=None) -> List[Dict[str, Any]]:
        """
        å˜ä¸€ã®æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Ÿè¡Œ
        
        Args:
            search_query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            max_results: æœ€å¤§å–å¾—çµæœæ•°
            
        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        search_results = []
        
        max_retries = GOOGLE_FALLBACK_MAX_RETRIES  # ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’æ¸›ã‚‰ã—ã¦å…¨ä½“ã®å‡¦ç†æ™‚é–“ã‚’çŸ­ç¸®
        retry_delay = GOOGLE_FALLBACK_RETRY_DELAY  # å¾…æ©Ÿæ™‚é–“
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        pattern_start_time = time.time()
        pattern_timeout = GOOGLE_PATTERN_TIMEOUT
        
        for retry in range(max_retries):
            # ãƒ‘ã‚¿ãƒ¼ãƒ³å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            if time.time() - pattern_start_time > pattern_timeout:
                print(f"    âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³å…¨ä½“ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{pattern_timeout}ç§’ï¼‰ã®ãŸã‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™")
                break
            try:
                # Googleæ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆUser-Agentè¨­å®šä»˜ãï¼‰
                search_urls = []
                
                # googlesearch-pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«è¿½åŠ ã®è¨­å®šã‚’è©¦è¡Œ
                try:
                    # ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“ã§ã‚ˆã‚Šäººé–“ã‚‰ã—ã„å‹•ä½œã‚’æ¨¡å€£
                    base_delay = max(self.delay * GOOGLE_FALLBACK_DELAY_MULTIPLIER, GOOGLE_MIN_DELAY)  # æœ€ä½å¾…æ©Ÿæ™‚é–“
                    
                    print(f"      ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢å®Ÿè¡Œä¸­ï¼ˆ{base_delay}ç§’é–“éš”ã§æ…é‡ã«å–å¾—ï¼‰...")
                    
                    # googlesearch-pythonã®User-Agentè¨­å®šã‚’è©¦è¡Œ
                    import googlesearch
                    
                    # ã‚ˆã‚Šä¿å®ˆçš„ãªè¨­å®šã§æ¤œç´¢ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–å¼·åŒ–ï¼‰
                    search_timeout = False
                    start_search_time = time.time()
                    
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã®æ¤œç´¢ã‚’å®Ÿè¡Œ
                    search_results_iter = None
                    try:
                        # concurrent.futuresã‚’ä½¿ã£ã¦æ¤œç´¢ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
                        import concurrent.futures
                        import signal
                        
                        def search_with_timeout():
                            try:
                                return list(search(
                                    search_query, 
                                    num=max_results, 
                                    stop=max_results, 
                                    pause=base_delay,
                                    lang='ja',  # æ—¥æœ¬èªæ¤œç´¢ã‚’æ˜ç¤º
                                    safe='off',  # ã‚»ãƒ¼ãƒ•ã‚µãƒ¼ãƒã‚ªãƒ•
                                    country='jp'  # æ—¥æœ¬ã‹ã‚‰ã®æ¤œç´¢ã‚’æ˜ç¤º
                                ))
                            except Exception as e:
                                print(f"    æ¤œç´¢é–¢æ•°å†…ã‚¨ãƒ©ãƒ¼: {e}")
                                return []
                        
                        # 60ç§’ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§æ¤œç´¢å®Ÿè¡Œ
                        with concurrent.futures.ThreadPoolExecutor(max_workers=THREAD_POOL_MAX_WORKERS_SINGLE) as executor:
                            future = executor.submit(search_with_timeout)
                            try:
                                search_results = future.result(timeout=GOOGLE_SEARCH_TIMEOUT)
                                print(f"      {len(search_results)}ä»¶ã®URLã‚’å–å¾—")
                                
                                for url in search_results:
                                    search_urls.append(url)
                                    print(f"    URLå–å¾—: {url}")
                                    
                                    if len(search_urls) >= max_results:
                                        break
                                    
                                    # é©åº¦ãªå¾…æ©Ÿ
                                    time.sleep(GOOGLE_URL_FETCH_DELAY)
                                    
                            except concurrent.futures.TimeoutError:
                                print(f"    âš ï¸ æ¤œç´¢å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{GOOGLE_SEARCH_TIMEOUT}ç§’ï¼‰ã®ãŸã‚ä¸­æ–­ã—ã¾ã™")
                                search_timeout = True
                                future.cancel()
                                
                    except Exception as search_setup_error:
                        print(f"    æ¤œç´¢ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {search_setup_error}")
                        search_timeout = True
                    
                    if search_timeout:
                        print(f"    âš ï¸ æ¤œç´¢ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚å–å¾—æ¸ˆã¿URL: {len(search_urls)}ä»¶")
                        if logger:
                            logger.log_error("google_search_timeout", f"æ¤œç´¢å‡¦ç†ãŒ{GOOGLE_SEARCH_TIMEOUT}ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ", {
                                "search_query": search_query,
                                "urls_collected": len(search_urls)
                            })
                        
                except Exception as search_lib_error:
                    print(f"    googlesearch-pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¨ãƒ©ãƒ¼: {search_lib_error}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç›´æ¥Googleæ¤œç´¢ã‚’è©¦è¡Œ
                    search_urls = self._fallback_google_search(search_query, max_results)
                
                # å„URLã‹ã‚‰æƒ…å ±ã‚’å–å¾—
                for url in search_urls:
                    try:
                        # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—
                        content_info = self._extract_page_content(url, character_name, api_key, logger)
                        if content_info:
                            search_results.append(content_info)
                        
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã§å¤§å¹…å¾…æ©Ÿ
                        time.sleep(self.delay * GOOGLE_PAGE_DELAY_MULTIPLIER)
                        
                    except Exception as e:
                        print(f"        âš ï¸ URLå–å¾—ã‚¨ãƒ©ãƒ¼: {url[:50]}... - {e}")
                        if logger:
                            logger.log_error("url_extraction_error", str(e), {"url": url, "search_query": search_query})
                        continue
                
                # æˆåŠŸã—ãŸå ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                break
                        
            except Exception as search_error:
                error_str = str(search_error)
                print(f"æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({search_query}): {search_error}")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                if str(HTTP_STATUS_TOO_MANY_REQUESTS) in error_str or "Too Many Requests" in error_str or "429" in error_str:
                    if logger:
                        logger.log_error("google_rate_limit", f"Googleæ¤œç´¢ãƒ¬ãƒ¼ãƒˆåˆ¶é™ (è©¦è¡Œ{retry+1}/{max_retries})", {
                            "search_query": search_query,
                            "retry_attempt": retry + 1,
                            "max_retries": max_retries,
                            "error_details": error_str
                        })
                        
                    print(f"âš ï¸  Googleæ¤œç´¢ã§429ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼‰ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                    print(f"    æ¤œç´¢ã‚¯ã‚¨ãƒª: {search_query}")
                    print(f"    è©¦è¡Œå›æ•°: {retry + 1}/{max_retries}")
                    
                    if retry < max_retries - 1:
                        # 429ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç‰¹åˆ¥ã«é•·ã„å¾…æ©Ÿæ™‚é–“ã‚’è¨­å®š
                        wait_time = GOOGLE_429_EXTRA_DELAY + (retry_delay * (retry + 1))
                        print(f"    429ã‚¨ãƒ©ãƒ¼ã®ãŸã‚{wait_time}ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        print(f"    ğŸ’¡ é »ç¹ã«ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’ãŠè©¦ã—ãã ã•ã„:")
                        print(f"       - --use-bing ãƒ•ãƒ©ã‚°ã§Bingæ¤œç´¢ã‚’ä½¿ç”¨")
                        print(f"       - --use-chatgpt-search ãƒ•ãƒ©ã‚°ã§ChatGPTçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨")
                        print(f"       - Google Custom Search APIã®è¨­å®šï¼ˆæ¨å¥¨ï¼‰")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸã€‚æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {search_query}")
                        print(f"    ğŸ’¡ Googleæ¤œç´¢ãŒç¶™ç¶šçš„ã«å¤±æ•—ã™ã‚‹å ´åˆã®å¯¾å‡¦æ³•:")
                        print(f"       1. --use-bing ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ã¦Bingæ¤œç´¢ã«åˆ‡ã‚Šæ›¿ãˆ")
                        print(f"       2. --no-google ãƒ•ãƒ©ã‚°ã§Webæ¤œç´¢ã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ–")
                        print(f"       3. æ•°æ™‚é–“å¾Œã«å†è©¦è¡Œï¼ˆGoogleå´ã®åˆ¶é™ãƒªã‚»ãƒƒãƒˆå¾…ã¡ï¼‰")
                        if logger:
                            logger.log_error("google_rate_limit_exceeded", f"Googleæ¤œç´¢ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¶…é", {
                                "search_query": search_query,
                                "max_retries": max_retries
                            })
                        break
                else:
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒªãƒˆãƒ©ã‚¤ã—ãªã„
                    if logger:
                        logger.log_error("google_search_error", str(search_error), {"search_query": search_query})
                    break
        
        return search_results
    
    def _fallback_google_search(self, query: str, max_results: int) -> List[str]:
        """
        googlesearch-pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            max_results: æœ€å¤§çµæœæ•°
            
        Returns:
            æ¤œç´¢çµæœURLã®ãƒªã‚¹ãƒˆ
        """
        print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œä¸­: {query}")
        urls = []
        
        try:
            # åŸºæœ¬çš„ãªGoogleæ¤œç´¢URLæ§‹ç¯‰
            import urllib.parse
            encoded_query = urllib.parse.quote_plus(query)
            google_url = f"https://www.google.com/search?q={encoded_query}&num={max_results}&hl=ja"
            
            print(f"    ç›´æ¥Googleæ¤œç´¢URL: {google_url}")
            
            response = self.session.get(google_url, timeout=REQUEST_TIMEOUT)
            
            if response.status_code == HTTP_STATUS_OK:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Googleæ¤œç´¢çµæœã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                for link in soup.find_all('a'):
                    href = link.get('href')
                    if href and href.startswith('/url?q='):
                        # Googleã®å†…éƒ¨URLã‹ã‚‰å®Ÿéš›ã®URLã‚’æŠ½å‡º
                        actual_url = href.split('/url?q=')[1].split('&')[0]
                        actual_url = urllib.parse.unquote(actual_url)
                        
                        # æœ‰åŠ¹ãªURLã®ã¿ã‚’è¿½åŠ 
                        if actual_url.startswith('http') and 'google.com' not in actual_url:
                            urls.append(actual_url)
                            if len(urls) >= max_results:
                                break
                
                print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢çµæœ: {len(urls)}ä»¶")
            else:
                print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢å¤±æ•—: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return urls
    
    def _extract_page_content(self, url: str, character_name: str = "", api_key: str = None, logger=None) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒšãƒ¼ã‚¸å†…å®¹ã‚’æŠ½å‡º
        
        Args:
            url: å¯¾è±¡URL
            
        Returns:
            æŠ½å‡ºã—ãŸå†…å®¹ã®è¾æ›¸
        """
        try:
            # URLã®æ¤œè¨¼ã¨ä¿®æ­£
            if not url.startswith(('http://', 'https://')):
                if url.startswith('//'):
                    url = 'https:' + url
                elif url.startswith('/'):
                    return None  # ç›¸å¯¾URLã¯ç„¡åŠ¹ã¨ã—ã¦é™¤å¤–
                else:
                    url = 'https://' + url
            
            # å…±é€šHTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            from utils.http_client import safe_http_get
            response = safe_http_get(url, max_retries=MAX_RETRIES, timeout=REQUEST_TIMEOUT, logger=None, quiet=True)  # ä¸€èˆ¬çš„ãªHTTPã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ã«è¨˜éŒ²ã›ãšã€å‡ºåŠ›ã‚‚æŠ‘åˆ¶
            
            if not response:
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            domain = self._extract_domain(url)  # ãƒ‰ãƒ¡ã‚¤ãƒ³æƒ…å ±ã‚’å…ˆã«å–å¾—
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "ä¸æ˜"
            
            # ãƒ¡ã‚¿èª¬æ˜ã‚’å–å¾—
            meta_description = soup.find('meta', attrs={'name': 'description'})
            description = meta_description.get('content', '') if meta_description else ''
            
            # æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆä¸»è¦ãªéƒ¨åˆ†ã®ã¿ï¼‰
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
            for script in soup(["script", "style", "nav", "header", "footer"]):
                script.decompose()
            
            # æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            body_text = soup.get_text()
            # ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»
            lines = (line.strip() for line in body_text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            body_text = ' '.join(chunk for chunk in chunks if chunk)
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¶é™
            body_text = body_text[:GOOGLE_PAGE_LIMIT] if body_text else ""
            
            # å£èª¿ãƒ»ã‚»ãƒªãƒ•é–¢é€£ã®æ–‡ã‚’ChatGPT APIã§æŠ½å‡ºï¼ˆAPI keyãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
            speech_patterns = []
            if api_key and len(body_text.strip()) > GOOGLE_MIN_TEXT_LENGTH_FOR_API:  # ååˆ†ãªãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿
                try:
                    openai_client = OpenAIClient(api_key)
                    speech_patterns = openai_client.extract_speech_patterns(body_text, character_name, logger)
                except Exception as api_error:
                    print(f"    APIæŠ½å‡ºã‚¹ã‚­ãƒƒãƒ— ({domain}): {api_error}")
                    if logger:
                        logger.log_error("speech_pattern_api_error", str(api_error), {
                            "url": url,
                            "domain": domain,
                            "character_name": character_name,
                            "content_length": len(body_text)
                        })
                    speech_patterns = []
            
            return {
                "url": url,
                "domain": domain,
                "title": title_text,
                "description": description,
                "content": body_text,
                "content_length": len(body_text),
                "speech_patterns": speech_patterns
            }
            
        except requests.RequestException as e:
            print(f"HTTPå–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            if logger:
                logger.log_error("http_request_error", str(e), {"url": url, "error_type": "RequestException"})
            return None
        except Exception as e:
            print(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            if logger:
                logger.log_error("content_extraction_error", str(e), {"url": url, "error_type": type(e).__name__})
            return None
    
    def search_youtube_videos(self, name: str) -> List[str]:
        """
        YouTubeå‹•ç”»ã‚’æ¤œç´¢ï¼ˆå‹•ç”»URLã‚’å„ªå…ˆçš„ã«å–å¾—ï¼‰
        
        Args:
            name: æ¤œç´¢å¯¾è±¡ã®åå‰
            
        Returns:
            YouTubeå‹•ç”»URLã®ãƒªã‚¹ãƒˆ
        """
        try:
            # ã‚ˆã‚Šç²¾å¯†ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
            search_queries = []
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã®ã¿ã®å˜ç´”ãªæ¤œç´¢
            if len(name) > BING_NAME_LENGTH_CHECK:  # åå‰ãŒçŸ­ã™ãã‚‹å ´åˆã®èª¤ãƒ’ãƒƒãƒˆé˜²æ­¢
                search_queries.extend([
                    f'"{name}" site:youtube.com'
                ])
            
            # ä¸€èˆ¬çš„ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã®ã¿ï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç‰¹æœ‰ã®æƒ…å ±ã¯å«ã‚ãªã„ï¼‰
            
            # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å»ƒæ­¢ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šï¼‰
            
            youtube_urls = []
            
            for search_query in search_queries:
                if len(youtube_urls) >= YOUTUBE_MAX_URLS:
                    break
                    
                print(f"YouTubeæ¤œç´¢ä¸­: {search_query}")
                
                try:
                    # Custom Search APIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯APIã‚’ä½¿ç”¨
                    if self.google_api_key and self.google_cx:
                        api_urls = self._search_youtube_with_api(search_query)
                        for url in api_urls:
                            if url not in youtube_urls:
                                youtube_urls.append(url)
                                print(f"  - å‹•ç”»URLç™ºè¦‹ï¼ˆAPIï¼‰: {url}")
                            if len(youtube_urls) >= YOUTUBE_MAX_URLS:
                                break
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ¤œç´¢æ–¹æ³•
                        for url in search(search_query):
                            # å‹•ç”»URLã‚’åé›†
                            if 'youtube.com/watch?v=' in url and url not in youtube_urls:
                                youtube_urls.append(url)
                                print(f"  - å‹•ç”»URLç™ºè¦‹: {url}")
                            
                            if len(youtube_urls) >= YOUTUBE_MAX_URLS:
                                break
                            time.sleep(YOUTUBE_SEARCH_DELAY)
                        
                except Exception as search_error:
                    print(f"YouTubeæ¤œç´¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({search_query}): {search_error}")
                    # YouTubeæ¤œç´¢ã‚¨ãƒ©ãƒ¼ã¯loggerã«è¨˜éŒ²ã—ãªã„ï¼ˆé€šå¸¸ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ãªã„ãŸã‚ï¼‰
                    continue
                
                # ã‚¯ã‚¨ãƒªé–“ã®å¤§å¹…å¾…æ©Ÿï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰
                time.sleep(self.delay * GOOGLE_YOUTUBE_DELAY_MULTIPLIER)
            
            print(f"YouTubeå‹•ç”»URLå–å¾—å®Œäº†: {len(youtube_urls)}ä»¶")
            return youtube_urls
            
        except Exception as e:
            print(f"YouTubeæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            # YouTubeæ¤œç´¢ã‚¨ãƒ©ãƒ¼ã¯loggerã«è¨˜éŒ²ã—ãªã„ï¼ˆé€šå¸¸ã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ãªã„ãŸã‚ï¼‰
            return []
    
    def _search_youtube_with_api(self, search_query: str) -> List[str]:
        """
        Custom Search APIã‚’ä½¿ç”¨ã—ã¦YouTubeå‹•ç”»ã‚’æ¤œç´¢
        
        Args:
            search_query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            
        Returns:
            YouTubeå‹•ç”»URLã®ãƒªã‚¹ãƒˆ
        """
        youtube_urls = []
        
        try:
            # Custom Search API ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆYouTubeæ¤œç´¢ï¼‰
            params = {
                'key': self.google_api_key,
                'cx': self.google_cx,
                'q': search_query,
                'num': GOOGLE_YOUTUBE_API_RESULTS,  # APIã§ã®å–å¾—ä»¶æ•°
                'lr': 'lang_ja',  # æ—¥æœ¬èªçµæœã‚’å„ªå…ˆ
                'gl': 'jp',  # æ—¥æœ¬ã‹ã‚‰ã®æ¤œç´¢ã¨ã—ã¦å®Ÿè¡Œ
                'safe': 'off',
                'siteSearch': 'youtube.com'  # YouTubeé™å®šæ¤œç´¢
            }
            
            response = self.session.get(self.api_base_url, params=params, timeout=REQUEST_TIMEOUT)
            
            if response.status_code == HTTP_STATUS_OK:
                data = response.json()
                
                if 'items' in data:
                    for item in data['items']:
                        url = item.get('link', '')
                        if 'youtube.com/watch?v=' in url:
                            youtube_urls.append(url)
                        
                        if len(youtube_urls) >= YOUTUBE_MAX_URLS:
                            break
                            
        except Exception as e:
            print(f"    YouTube APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return youtube_urls
    
    def _extract_title_from_url(self, url: str) -> str:
        """
        URLã‹ã‚‰ç°¡å˜ã«ã‚¿ã‚¤ãƒˆãƒ«æƒ…å ±ã‚’æŠ½å‡ºï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        
        Args:
            url: YouTube URL
            
        Returns:
            ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—åˆ—ï¼ˆå–å¾—ã§ããªã„å ´åˆã¯ç©ºæ–‡å­—ï¼‰
        """
        # å®Ÿéš›ã®ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ã¯é‡ã™ãã‚‹ã®ã§ã€ç©ºæ–‡å­—ã‚’è¿”ã™
        return ""
    
    
