"""
YouTubeå­—å¹•åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import re
import random
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
from typing import List, Dict, Any
from urllib.parse import urlparse, parse_qs
from core.interfaces import CharacterQuote
from config import (
    YOUTUBE_MAX_VIDEOS, YOUTUBE_MAX_TRANSCRIPTS, YOUTUBE_TRANSCRIPT_LIMIT,
    SAMPLE_PHRASES_MAX, SAMPLE_PHRASE_MIN_LENGTH, SAMPLE_PHRASE_MAX_LENGTH,
    SAMPLE_QUALITY_MIN_LENGTH, SAMPLE_QUALITY_MAX_LENGTH,
    CHATGPT_FILTER_TEXT_LIMIT, OPENAI_MODEL, OPENAI_FILTER_MAX_TOKENS,
    OPENAI_FILTER_TEMPERATURE, get_search_patterns,
    YOUTUBE_VIDEO_ID_DISPLAY_LENGTH, YOUTUBE_PREVIEW_TEXT_LENGTH,
    YOUTUBE_SAMPLE_DISPLAY_LIMIT, YOUTUBE_MIN_SENTENCE_LENGTH,
    YOUTUBE_MAX_SINGLE_CHAR_LENGTH, YOUTUBE_MAX_PERIOD_COUNT,
    YOUTUBE_FILTER_PHRASE_LIMIT, YOUTUBE_ANALYSIS_TEXT_LIMIT,
    YOUTUBE_ANALYSIS_MAX_TOKENS, REGEX_JAPANESE_CHAR_MIN, REGEX_JAPANESE_CHAR_MAX
)


class YouTubeCollector:
    """YouTubeå‹•ç”»ã®å­—å¹•ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.formatter = TextFormatter()
    
    def collect_info(self, youtube_urls: List[str], max_videos: int = None, logger=None, character_info: Dict = None, api_key: str = None) -> Dict[str, Any]:
        """
        YouTube URLã‹ã‚‰å­—å¹•æƒ…å ±ã‚’åé›†
        
        Args:
            youtube_urls: YouTube URLã®ãƒªã‚¹ãƒˆ
            max_videos: å‡¦ç†ã™ã‚‹æœ€å¤§å‹•ç”»æ•°
            
        Returns:
            åé›†ã—ãŸå­—å¹•æƒ…å ±ã®è¾æ›¸
        """
        try:
            if max_videos is None:
                max_videos = YOUTUBE_MAX_VIDEOS
                
            if not youtube_urls:
                return {
                    "found": False,
                    "error": "YouTube URLãŒæä¾›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ",
                    "transcripts": [],
                    "total_videos": 0,
                    "sample_phrases": []
                }
            
            print(f"  {len(youtube_urls)}å€‹ã®å‹•ç”»ã‹ã‚‰å­—å¹•ã‚’åé›†ä¸­ï¼ˆæœ€å¤§{max_videos}å‹•ç”»ï¼‰...")
            
            transcripts = []
            all_text = []
            total_videos = min(len(youtube_urls), max_videos)
            
            for i, url in enumerate(youtube_urls[:max_videos]):
                video_id = self._extract_video_id(url)
                if not video_id:
                    print(f"  å‹•ç”»{i+1}: ç„¡åŠ¹ãªURL - {url}")
                    continue
                
                print(f"  å‹•ç”»{i+1}/{total_videos} (ID: {video_id[:YOUTUBE_VIDEO_ID_DISPLAY_LENGTH]}): å­—å¹•å–å¾—ä¸­...")
                transcript_info = self._get_video_transcript(video_id)
                
                if transcript_info["found"]:
                    transcripts.append(transcript_info)
                    all_text.append(transcript_info["text"])
                    # å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    preview = transcript_info["text"][:YOUTUBE_PREVIEW_TEXT_LENGTH].replace('\n', ' ')
                    print(f"    âœ… å­—å¹•å–å¾—æˆåŠŸ ({transcript_info['word_count']}èª, {transcript_info['language']})")
                    print(f"    ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {preview}...")
                else:
                    print(f"    âŒ å­—å¹•å–å¾—å¤±æ•—: {transcript_info['error']}")
                
                # å­—å¹•ãŒååˆ†å–å¾—ã§ããŸã‚‰æ—©æœŸçµ‚äº†
                if len(transcripts) >= YOUTUBE_MAX_TRANSCRIPTS:
                    print(f"  ååˆ†ãªå­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ ({len(transcripts)}å‹•ç”»)")
                    break
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡º
            print(f"\n  ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡ºä¸­...")
            sample_phrases = self._extract_sample_phrases(all_text)
            
            # ã‚µãƒ³ãƒ—ãƒ«å“è³ªãƒã‚§ãƒƒã‚¯
            quality_checked_phrases = self._check_sample_quality(sample_phrases)
            
            # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ç›¸å½“ã®è¨€èªç‰¹å¾´æŠ½å‡ºï¼ˆAPI keyãŒã‚ã‚‹å ´åˆï¼‰
            pattern_analysis = {}
            if api_key and all_text:
                print(f"  ğŸ¤” è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­ (ChatGPT API)...")
                pattern_analysis = self._analyze_speech_patterns(all_text, character_info.get("name", ""), api_key)
            
            print(f"  ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡º: {len(sample_phrases)}å€‹ â†’ å“è³ªãƒã‚§ãƒƒã‚¯å¾Œ: {len(quality_checked_phrases)}å€‹")
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šæŠ½å‡ºã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ã‚ºã®ä¸€éƒ¨ã‚’è¡¨ç¤º
            if quality_checked_phrases:
                print(f"  ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®{YOUTUBE_SAMPLE_DISPLAY_LIMIT}å€‹ï¼‰:")
                for i, phrase in enumerate(quality_checked_phrases[:YOUTUBE_SAMPLE_DISPLAY_LIMIT]):
                    print(f"    {i+1}. {phrase}")
            else:
                print("  âš ï¸ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚ºãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
            # CharacterQuoteã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ã‚»ãƒªãƒ•ã‚’æ•´ç†
            character_quotes = []
            for phrase in quality_checked_phrases:
                # YouTubeã‚½ãƒ¼ã‚¹ã¯åŸºæœ¬çš„ã«ä¿¡é ¼æ€§ã¯ä¸­ç¨‹åº¦
                # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åãŒã‚»ãƒªãƒ•ã«å«ã¾ã‚Œã‚‹ã‹ã€APIã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¿¡é ¼æ€§ã‚’ä¸Šã’ã‚‹
                confidence = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¿¡é ¼æ€§
                character_name = character_info.get("name", "") if character_info else ""
                
                # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åãŒã‚»ãƒªãƒ•ã«å«ã¾ã‚Œã‚‹å ´åˆã¯ä¿¡é ¼æ€§ã‚¢ãƒƒãƒ—
                if character_name and character_name in phrase:
                    confidence = 0.8
                
                quote = CharacterQuote(
                    text=phrase,
                    source="youtube",
                    source_url=None,  # è¤‡æ•°ã®å‹•ç”»ã‹ã‚‰é›†ç´„ã•ã‚Œã¦ã„ã‚‹ãŸã‚
                    confidence_score=confidence,
                    context="YouTubeå‹•ç”»å­—å¹•ã‹ã‚‰ChatGPT APIã§æŠ½å‡º"
                )
                # è¾æ›¸å½¢å¼ã«å¤‰æ›ã—ã¦ã‹ã‚‰è¿½åŠ ï¼ˆJSONä¿å­˜ã®ãŸã‚ï¼‰
                character_quotes.append(quote.to_dict())
            
            return {
                "found": len(transcripts) > 0,
                "error": None if len(transcripts) > 0 else "å­—å¹•ä»˜ãå‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                "transcripts": transcripts,
                "total_videos": len(transcripts),
                "sample_phrases": quality_checked_phrases,
                "character_quotes": character_quotes,  # CharacterQuoteã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã‚’è¿½åŠ 
                "processed_urls": len(youtube_urls),
                "successful_extractions": len(transcripts),
                "speech_pattern_analysis": pattern_analysis  # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ç›¸å½“ã®åˆ†æçµæœ
            }
            
        except Exception as e:
            import traceback
            error_msg = f"YouTubeå­—å¹•åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            error_traceback = traceback.format_exc()
            
            print(f"âŒ YouTubeå­—å¹•åé›†ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
            print(f"ã‚¨ãƒ©ãƒ¼: {error_msg}")
            print(f"ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{error_traceback}")
            
            return {
                "found": False,
                "error": error_msg,
                "transcripts": [],
                "total_videos": 0,
                "sample_phrases": []
            }
    
    def _extract_video_id(self, url: str) -> str:
        """
        YouTubeã®URLã‹ã‚‰å‹•ç”»IDã‚’æŠ½å‡º
        
        Args:
            url: YouTube URL
            
        Returns:
            å‹•ç”»IDï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneï¼‰
        """
        try:
            # æ§˜ã€…ãªYouTube URLãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ
            patterns = [
                r'(?:youtube\.com/watch\?v=|youtu\.be/|youtube\.com/embed/)([^&\n?#]+)',
                r'youtube\.com/watch\?.*v=([^&\n?#]+)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    return match.group(1)
            
            return None
        except Exception:
            return None
    
    def _get_video_transcript(self, video_id: str) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸå‹•ç”»IDã®å­—å¹•ã‚’å–å¾—
        
        Args:
            video_id: YouTubeå‹•ç”»ID
            
        Returns:
            å­—å¹•æƒ…å ±ã®è¾æ›¸
        """
        try:
            # åˆ©ç”¨å¯èƒ½ãªå­—å¹•è¨€èªã‚’å–å¾—
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # æ—¥æœ¬èªå­—å¹•ã‚’å„ªå…ˆçš„ã«å–å¾—
            transcript = None
            try:
                transcript = transcript_list.find_transcript(['ja', 'jp'])
            except:
                # æ—¥æœ¬èªãŒãªã„å ´åˆã¯è‹±èªã‚’è©¦ã™
                try:
                    transcript = transcript_list.find_transcript(['en'])
                except:
                    # è‡ªå‹•ç”Ÿæˆå­—å¹•ã‚’è©¦ã™
                    try:
                        transcript = transcript_list.find_generated_transcript(['ja', 'jp', 'en'])
                    except:
                        pass
            
            if not transcript:
                return {
                    "found": False,
                    "error": "åˆ©ç”¨å¯èƒ½ãªå­—å¹•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                    "video_id": video_id,
                    "text": "",
                    "language": None
                }
            
            # å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            transcript_data = transcript.fetch()
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆï¼ˆæ­£ã—ã„å±æ€§ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
            text_parts = []
            for entry in transcript_data:
                if hasattr(entry, 'text'):
                    text_parts.append(entry.text)
                elif isinstance(entry, dict) and 'text' in entry:
                    text_parts.append(entry['text'])
                else:
                    # FetchedTranscriptSnippetã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                    text_parts.append(str(entry))
            
            full_text = ' '.join(text_parts)
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¶é™
            limited_text = full_text[:YOUTUBE_TRANSCRIPT_LIMIT] if full_text else ""
            
            return {
                "found": True,
                "error": None,
                "video_id": video_id,
                "text": limited_text,
                "language": transcript.language_code,
                "is_generated": transcript.is_generated,
                "word_count": len(limited_text.split())
            }
            
        except Exception as e:
            return {
                "found": False,
                "error": f"å­—å¹•å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}",
                "video_id": video_id,
                "text": "",
                "language": None
            }
    
    def _extract_sample_phrases(self, text_list: List[str], max_phrases: int = None) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡ºï¼ˆå®Œå…¨ã«ä¸­ç«‹çš„ãªæŠ½å‡ºï¼‰
        
        Args:
            text_list: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            max_phrases: æŠ½å‡ºã™ã‚‹æœ€å¤§ãƒ•ãƒ¬ãƒ¼ã‚ºæ•°
            
        Returns:
            ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚ºã®ãƒªã‚¹ãƒˆ
        """
        try:
            if max_phrases is None:
                max_phrases = SAMPLE_PHRASES_MAX
                
            all_text = ' '.join(text_list)
            
            if not all_text:
                return []
            
            # ãƒã‚¤ã‚ºã‚’é™¤å»ï¼ˆ[éŸ³æ¥½]ã€[æ‹æ‰‹]ãªã©ã®è¨˜å·ï¼‰
            cleaned_text = re.sub(r'\[.*?\]', '', all_text)
            
            # æ–‡ç« ã‚’åˆ†å‰²
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]', cleaned_text)
            
            filtered_sentences = []
            
            for sentence in sentences:
                sentence = sentence.strip()
                
                # åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ã¿ï¼ˆç‰¹å¾´çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹å„ªå…ˆé †ä½ä»˜ã‘ã¯å»ƒæ­¢ï¼‰
                if (SAMPLE_PHRASE_MIN_LENGTH <= len(sentence) <= SAMPLE_PHRASE_MAX_LENGTH and 
                    not re.match(r'^[éŸ³æ¥½æ‹æ‰‹åŠ¹æœéŸ³]+', sentence) and
                    not re.match(rf'^[ã‚-ã‚“]{{{REGEX_JAPANESE_CHAR_MIN},{REGEX_JAPANESE_CHAR_MAX}}}$', sentence) and
                    sentence not in ['', ' ', 'ã†ã‚“', 'ãã†', 'ã¯ã„', 'ãˆãƒ¼', 'ã‚ãƒ¼']):
                    
                    filtered_sentences.append(sentence)
            
            # é‡è¤‡ã‚’é™¤å»
            unique_sentences = []
            seen = set()
            for sentence in filtered_sentences:
                # æ­£è¦åŒ–ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯
                normalized = re.sub(r'\s+', '', sentence.lower())  # ç©ºç™½é™¤å»ï¼‹å°æ–‡å­—åŒ–
                if normalized not in seen and len(normalized) > SAMPLE_PHRASE_MIN_LENGTH:
                    unique_sentences.append(sentence)
                    seen.add(normalized)
            
            # ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠï¼ˆå„ªå…ˆé †ä½ãªã—ï¼‰
            if len(unique_sentences) > max_phrases:
                return random.sample(unique_sentences, max_phrases)
            else:
                return unique_sentences
            
        except Exception as e:
            print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _check_sample_quality(self, sample_phrases: List[str]) -> List[str]:
        """
        ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚ºã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            sample_phrases: æŠ½å‡ºã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚ºã®ãƒªã‚¹ãƒˆ
            
        Returns:
            å“è³ªãƒã‚§ãƒƒã‚¯æ¸ˆã¿ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ãƒ¬ãƒ¼ã‚º
        """
        try:
            quality_phrases = []
            
            # å“è³ªåŸºæº–
            for phrase in sample_phrases:
                phrase = phrase.strip()
                
                # åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
                if (len(phrase) < SAMPLE_QUALITY_MIN_LENGTH or len(phrase) > SAMPLE_QUALITY_MAX_LENGTH or
                    phrase.count('è©°ã‚“ã ã‚ã†ãŒ') > 0 or  # æ˜ã‚‰ã‹ã«é–“é•ã£ãŸæ–‡ã‚’é™¤å¤–
                    phrase.count('æ•™ä¼šã®å¸¸è­˜') > 0 or
                    re.match(r'^[åŒã˜æ–‡ç« ã®ç¹°ã‚Šè¿”ã—]', phrase) or
                    phrase.count('ã€‚') > YOUTUBE_MAX_PERIOD_COUNT):  # è¤‡æ•°æ–‡ãŒæ··åœ¨ã—ã¦ã„ã‚‹å ´åˆ
                    continue
                
                # æ„å‘³ã®ã‚ã‚‹æ–‡ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
                if (not re.match(r'^[ã‚ã„ã†ãˆãŠã‹ããã‘ã“]+$', phrase) and  # æ„å‘³ã®ãªã„æ–‡å­—åˆ—
                    not re.match(r'^[éŸ³æ¥½åŠ¹æœéŸ³]+', phrase) and
                    phrase not in ['', ' ', 'ã¯ã„', 'ãã†', 'ã†ã‚“']):
                    quality_phrases.append(phrase)
            
            return quality_phrases
            
        except Exception as e:
            print(f"ã‚µãƒ³ãƒ—ãƒ«å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return sample_phrases  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    
    def filter_character_speech(self, text_list: List[str], character_name: str, api_key: str = None) -> List[str]:
        """
        å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å®šã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ç™ºè¨€ã‚’æŠ½å‡º
        
        Args:
            text_list: å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            character_name: å¯¾è±¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            api_key: OpenAI API Keyï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸç™ºè¨€ãƒªã‚¹ãƒˆ
        """
        if not api_key:
            # API KeyãŒãªã„å ´åˆã¯åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ã¿
            return self._extract_sample_phrases(text_list)
        
        try:
            import openai
            client = openai.OpenAI(api_key=api_key)
            
            all_text = ' '.join(text_list)
            if len(all_text) > CHATGPT_FILTER_TEXT_LIMIT:  # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
                all_text = all_text[:CHATGPT_FILTER_TEXT_LIMIT]
            
            print(f"  ChatGPT APIã§{character_name}ã®ç™ºè¨€ã‚’ç‰¹å®šä¸­...")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç‰¹æœ‰ã®ç‰¹å¾´ã‚’å«ã‚ã‚‹ï¼‰
            system_prompt = "ã‚ãªãŸã¯å­—å¹•ã‹ã‚‰ç‰¹å®šã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ç™ºè¨€ã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ç‰¹å¾´çš„ãªå£èª¿ã‚„èªå°¾ã‚’æ­£ç¢ºã«è­˜åˆ¥ã—ã€ä»–ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚„é–¢ä¿‚ãªã„ç™ºè¨€ã¯ç¢ºå®Ÿã«é™¤å¤–ã—ã¦ãã ã•ã„ã€‚"
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å›ºæœ‰ã®ç‰¹å¾´ã‚’è¿½åŠ 
            character_features = self._get_character_features(character_name)
            
            user_prompt = f"""
ä»¥ä¸‹ã®å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€Œ{character_name}ã€ãŒè©±ã—ã¦ã„ã‚‹éƒ¨åˆ†ã ã‘ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€{character_name}ã®åˆ†æå¯¾è±¡ç‰¹å¾´ã€‘
{character_features}

ã€æŠ½å‡ºåŸºæº–ã€‘
1. {character_name}ç‰¹æœ‰ã®èªå°¾ã‚„å£èª¿ã‚’å«ã‚€ç™ºè¨€ã‚’æŠ½å‡º
2. {character_name}ç‰¹æœ‰ã®ä¸€äººç§°ã‚’ä½¿ã£ãŸç™ºè¨€ã‚’æŠ½å‡º
3. {character_name}ã‚‰ã—ã„æ€§æ ¼ã‚„ç‰¹å¾´ã‚’ç¤ºã™ç™ºè¨€ã‚’æŠ½å‡º
4. æ˜ã‚‰ã‹ã«ä»–ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ç‰¹å¾´ã‚’æŒã¤ç™ºè¨€ã¯é™¤å¤–

ã€é™¤å¤–ã™ã¹ãç™ºè¨€ã€‘
- é–¢ä¿‚ã®ãªã„ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚„èª¬æ˜æ–‡
- [éŸ³æ¥½]ã‚„[æ‹æ‰‹]ãªã©ã®åŠ¹æœéŸ³
- æ˜ã‚‰ã‹ã«{character_name}ã¨ç„¡é–¢ä¿‚ãªä¼šè©±
- æŠ€è¡“çš„ãªèª¬æ˜ã‚„å®Ÿæ³ã‚³ãƒ¡ãƒ³ãƒˆ

ã€å‡ºåŠ›è¦æ±‚ã€‘
- {character_name}ã‚‰ã—ã„ç™ºè¨€ã®ã¿{YOUTUBE_FILTER_PHRASE_LIMIT}å€‹ä»¥å†…
- å„ç™ºè¨€ã‚’æ”¹è¡Œã§åŒºåˆ‡ã‚‹
- é‡è¤‡ã¯é¿ã‘ã‚‹
- ä¸ç¢ºå®Ÿãªå ´åˆã¯é™¤å¤–

å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆ:
{all_text}
"""
            
            response = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=OPENAI_FILTER_MAX_TOKENS,
                temperature=OPENAI_FILTER_TEMPERATURE
            )
            
            filtered_text = response.choices[0].message.content.strip()
            filtered_phrases = [phrase.strip() for phrase in filtered_text.split('\n') if phrase.strip()]
            
            print(f"  âœ… {len(filtered_phrases)}å€‹ã®{character_name}ã‚‰ã—ã„ç™ºè¨€ã‚’ç‰¹å®š")
            
            # APIã®ã‚„ã‚Šå–ã‚Šã‚‚è¿”ã™
            return {
                "filtered_phrases": filtered_phrases[:YOUTUBE_FILTER_PHRASE_LIMIT],
                "api_interaction": {
                    "system_prompt": system_prompt,
                    "user_prompt": user_prompt,
                    "response": filtered_text,
                    "model": OPENAI_MODEL,
                    "character_name": character_name
                }
            }
            
        except Exception as e:
            print(f"  âŒ ChatGPT APIã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            return {
                "filtered_phrases": self._extract_sample_phrases(text_list),
                "api_interaction": {
                    "error": str(e),
                    "fallback_used": True
                }
            }
    
    def _get_character_features(self, character_name: str) -> str:
        """
        ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å›ºæœ‰ã®ç‰¹å¾´ã‚’å–å¾—ï¼ˆä¸­ç«‹çš„ãªåˆ†ææŒ‡ç¤ºã®ã¿ï¼‰
        
        Args:
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            
        Returns:
            ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ç‰¹å¾´æ–‡å­—åˆ—
        """
        # äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç‰¹å¾´ã¯å»ƒæ­¢ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«ã‚ˆã‚Šï¼‰
        # å®Œå…¨ã«ä¸­ç«‹çš„ãªåˆ†ææŒ‡ç¤ºã®ã¿ã‚’æä¾›
        return f"""
- {character_name}ç‰¹æœ‰ã®ä¸€äººç§°ã‚„å‘¼ã³æ–¹
- {character_name}ç‰¹æœ‰ã®èªå°¾ã‚„å£èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³
- {character_name}ç‰¹æœ‰ã®æ€§æ ¼ã‚’è¡¨ã™è©±ã—æ–¹
- {character_name}ç‰¹æœ‰ã®è¡¨ç¾æ–¹æ³•ã‚„æ±ºã¾ã‚Šæ–‡å¥
- ä»–ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨åŒºåˆ¥ã•ã‚Œã‚‹è¨€èªçš„ç‰¹å¾´
"""
    
    def _analyze_speech_patterns(self, text: str, character_name: str, api_key: str) -> Dict[str, Any]:
        """
        YouTubeå­—å¹•ã‹ã‚‰æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ç›¸å½“ã®è¨€èªç‰¹å¾´ã‚’åˆ†æ
        
        Args:
            text: å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆ
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            api_key: OpenAI API Key
            
        Returns:
            åˆ†æçµæœã®è¾æ›¸
        """
        try:
            if not text or not api_key:
                return {}
            
            # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
            if len(text) > YOUTUBE_ANALYSIS_TEXT_LIMIT:
                text = text[:YOUTUBE_ANALYSIS_TEXT_LIMIT]
            
            import openai
            client = openai.OpenAI(api_key=api_key)
            
            # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
            search_patterns = get_search_patterns(character_name)
            pattern_descriptions = [
                "äººç‰©ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ»åŸºæœ¬æƒ…å ±",
                "åå°è©ãƒ»æ±ºã¾ã‚Šæ–‡å¥",
                "ã‚»ãƒªãƒ•ãƒ»ç™ºè¨€ä¾‹",
                "å£ç™–ãƒ»èªå°¾",
                "è©±ã—æ–¹ãƒ»ç‰¹å¾´",
                "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ»æ€§æ ¼",
                "ä¸€äººç§°ãƒ»å‘¼ã³æ–¹",
                "æ±ºã¾ã‚Šæ–‡å¥ãƒ»å¸¸å¥—å¥",
                "ç‰¹å¾´ãƒ»ç‰¹æ€§",
                "è§£èª¬ãƒ»ã¾ã¨ã‚"
            ]
            
            system_prompt = """ã‚ãªãŸã¯å‹•ç”»å­—å¹•ã‹ã‚‰è©±è€…ã®è¨€èªçš„ç‰¹å¾´ã‚’ä¸­ç«‹çš„ã«åˆ†æã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
è©±è€…ã®ç‰¹å®šã«æ³¨æ„ã‚’æ‰•ã„ã€æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ç™ºè¨€ã®ã¿ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚"""
            
            user_prompt = f"""ä»¥ä¸‹ã®å‹•ç”»å­—å¹•ã‹ã‚‰ã€Œ{character_name}ã€ã®è¨€èªçš„ç‰¹å¾´ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ã€‘è©±è€…ã®ç‰¹å®šã«ã¤ã„ã¦ï¼š
- è¤‡æ•°ã®è©±è€…ãŒæ··åœ¨ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- {character_name}ã®ç™ºè¨€ã¨åˆ¤æ–­ã§ãã‚‹ã‚‚ã®ã®ã¿ã‚’åˆ†æå¯¾è±¡ã¨ã™ã‚‹
- ä¸æ˜ç¢ºãªå ´åˆã¯ã€Œä¸æ˜ã€ã¨ã—ã¦æ‰±ã†

ã€åˆ†æé …ç›®ã€‘
ä»¥ä¸‹ã®å„é …ç›®ã«ã¤ã„ã¦ã€å­—å¹•ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
1. äººç‰©ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ»åŸºæœ¬æƒ…å ±
2. åå°è©ãƒ»æ±ºã¾ã‚Šæ–‡å¥
3. ã‚»ãƒªãƒ•ãƒ»ç™ºè¨€ä¾‹
4. å£ç™–ãƒ»èªå°¾ï¼ˆã‚ã‚‰ã‚†ã‚‹å½¢ã®èªå°¾ã‚’è¦‹é€ƒã•ãšã«æŠ½å‡ºï¼‰
5. è©±ã—æ–¹ãƒ»ç‰¹å¾´
6. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ»æ€§æ ¼
7. ä¸€äººç§°ãƒ»å‘¼ã³æ–¹
8. æ±ºã¾ã‚Šæ–‡å¥ãƒ»å¸¸å¥—å¥
9. ç‰¹å¾´ãƒ»ç‰¹æ€§
10. è§£èª¬ãƒ»ã¾ã¨ã‚

ã€èªå°¾ã«ã¤ã„ã¦ã€‘
- ã‚ã‚‰ã‚†ã‚‹å½¢ã®èªå°¾ã‚’è¦‹é€ƒã•ãšã«æŠ½å‡º
- çŸ­ã„èªå°¾ã€é•·ã„èªå°¾ã€çã—ã„èªå°¾ã‚‚å«ã‚€
- å¤‰åŒ–å½¢ã‚‚å«ã‚€
- å¥èª­ç‚¹ã¨ã®çµ„ã¿åˆã‚ã›ã‚‚æ­£ç¢ºã«è¨˜éŒ²

ã€åŸå‰‡ã€‘
- å­—å¹•ã«å®Ÿéš›ã«å«ã¾ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’æŠ½å‡º
- {character_name}ä»¥å¤–ã®è©±è€…ã®ç™ºè¨€ã¯é™¤å¤–
- æ¨æ¸¬ã‚„ä¸€èˆ¬çš„ãªçŸ¥è­˜ã¯ä½¿ç”¨ã—ãªã„
- ç‰¹æ®Šèªå°¾ã‚„çã—ã„èªå°¾ã‚‚è¦‹é€ƒã•ãšã«æŠ½å‡º
- èªå°¾ã®å…¨ã¦ã®å¤‰åŒ–å½¢ã‚’å«ã‚ã¦æŠ½å‡º
- è¦‹ã¤ã‹ã‚‰ãªã„é …ç›®ã¯ã€Œãªã—ã€ã¨è¨˜è¼‰

ã€å‡ºåŠ›å½¢å¼ã€‘
å„é …ç›®ã‚’ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ï¼š
é …ç›®1: [æŠ½å‡ºã•ã‚ŒãŸå†…å®¹ã¾ãŸã¯ãªã—]
é …ç›®2: [æŠ½å‡ºã•ã‚ŒãŸå†…å®¹ã¾ãŸã¯ãªã—]
...

å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆ:
{text}"""
            
            response = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=YOUTUBE_ANALYSIS_MAX_TOKENS,
                temperature=OPENAI_FILTER_TEMPERATURE
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # çµæœã‚’è§£æã—ã¦æ§‹é€ åŒ–
            analysis_result = {}
            lines = result_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if ':' in line and line.startswith('é …ç›®'):
                    try:
                        # "é …ç›®1: å†…å®¹" ã®å½¢å¼ã‹ã‚‰æŠ½å‡º
                        parts = line.split(':', 1)
                        if len(parts) == 2:
                            item_num = parts[0].replace('é …ç›®', '').strip()
                            content = parts[1].strip()
                            if content and content != 'ãªã—':
                                analysis_result[f"pattern_{item_num}"] = content
                    except:
                        continue
            
            print(f"  ğŸ¯ YouTubeè¨€èªç‰¹å¾´åˆ†æ: {len(analysis_result)}é …ç›®æŠ½å‡º")
            
            return analysis_result
            
        except Exception as e:
            print(f"YouTubeè¨€èªç‰¹å¾´åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}